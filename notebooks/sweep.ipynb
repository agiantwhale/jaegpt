{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Jae-GPT GPU Sweep",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWSJpsyKqHjH",
        "outputId": "fde4bf68-1b60-4b10-9579-7aecc23be6e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl-oT5N8VwFq",
        "outputId": "b35b6fd1-c70d-4ca9-87a5-db7d29f43136"
      },
      "source": [
        "!apt install htop\n",
        "!pip install wandb \\\n",
        "  transformers==4.8.2 \\\n",
        "  torch==1.9.0 \\\n",
        "  datasets==1.8.0 \\\n",
        "  gpustat\n",
        "!git clone https://github.com/agiantwhale/jaegpt.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "htop is already the newest version (2.1.0-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.10.33)\n",
            "Requirement already satisfied: transformers==4.8.2 in /usr/local/lib/python3.7/dist-packages (4.8.2)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: datasets==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n",
            "Requirement already satisfied: gpustat in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.1)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.0.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.18)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: sentry-sdk>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.6.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (0.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.8.2) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.7.4.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.3.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (0.70.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (2021.7.0)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.8.0) (3.0.0)\n",
            "Requirement already satisfied: blessings>=1.6 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.7)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.8.2) (3.5.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.8.2) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.8.2) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.8.0) (2018.9)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n",
            "fatal: destination path 'jaegpt' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDv6jfUGT8Wb"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xok-Le00SK1n",
        "outputId": "27379431-93a4-4ec7-cbff-ba23c83300bb"
      },
      "source": [
        "wandb.login(key='')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magiantwhale\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publically.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxQY40AQIzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74762a25-e1b2-4794-9b83-5e527e354417"
      },
      "source": [
        "!wandb agent agiantwhale/jaegpt-jaegpt/mwynodig"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Starting wandb agent 🕵️\n",
            "2021-07-15 19:32:20,257 - wandb.wandb_agent - INFO - Running runs: []\n",
            "2021-07-15 19:32:20,517 - wandb.wandb_agent - INFO - Agent received command: run\n",
            "2021-07-15 19:32:20,517 - wandb.wandb_agent - INFO - Agent starting run with config:\n",
            "\tblock_size: 128\n",
            "\tdataloader_num_workers: 4\n",
            "\tdo_eval: True\n",
            "\tdo_train: True\n",
            "\tevaluation_strategy: no\n",
            "\tgradient_accumulation_steps: 1\n",
            "\tlearning_rate: 0.003939800158469356\n",
            "\tlm_task_weight: 1\n",
            "\tlogging_strategy: no\n",
            "\tmc_task_weight: 1.9363834633606614\n",
            "\tmodel_name_or_path: microsoft/DialoGPT-small\n",
            "\toutput_dir: /tmp/hp_search\n",
            "\toverwrite_output_dir: True\n",
            "\tper_device_eval_batch_size: 16\n",
            "\tper_device_train_batch_size: 1\n",
            "\tpreprocessing_num_workers: 4\n",
            "\treport_to: wandb\n",
            "\tsave_strategy: no\n",
            "\tseed: 1337\n",
            "\ttrain_file: /content/drive/MyDrive/ColabData/input_data/fb_train.json\n",
            "\tvalidation_file: /content/drive/MyDrive/ColabData/input_data/fb_test.json\n",
            "2021-07-15 19:32:20,521 - wandb.wandb_agent - INFO - About to run command: /usr/bin/env python jaegpt/run_clm.py --block_size=128 --dataloader_num_workers=4 --do_eval=True --do_train=True --evaluation_strategy=no --gradient_accumulation_steps=1 --learning_rate=0.003939800158469356 --lm_task_weight=1 --logging_strategy=no --mc_task_weight=1.9363834633606614 --model_name_or_path=microsoft/DialoGPT-small --output_dir=/tmp/hp_search --overwrite_output_dir=True --per_device_eval_batch_size=16 --per_device_train_batch_size=1 --preprocessing_num_workers=4 --report_to=wandb --save_strategy=no --seed=1337 --train_file=/content/drive/MyDrive/ColabData/input_data/fb_train.json --validation_file=/content/drive/MyDrive/ColabData/input_data/fb_test.json\n",
            "2021-07-15 19:32:22.561078: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/15/2021 19:32:24 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/15/2021 19:32:24 - INFO - __main__ -   Training/evaluation parameters FBGPTTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=4,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.003939800158469356,\n",
            "length_column_name=length,\n",
            "lm_task_weight=1.0,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/tmp/hp_search/runs/Jul15_19-32-24_56450babe7f6,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.NO,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "mc_task_weight=1.9363834633606614,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=/tmp/hp_search,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=hp_search,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/tmp/hp_search,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=1337,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/15/2021 19:32:25 - WARNING - datasets.builder -   Using custom data configuration default-1f1027bf6e73f1ee\n",
            "07/15/2021 19:32:25 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02)\n",
            "2021-07-15 19:32:25,535 - wandb.wandb_agent - INFO - Running runs: ['s5d8hrdv']\n",
            "[INFO|configuration_utils.py:530] 2021-07-15 19:32:25,590 >> loading configuration file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0cbdd50f204f3ddbaa452e976340a5725f0b5ddb201704058c87e14d9679e070.e6898db50ba3aa698f0f652e876a1e4bd813321dea3e22b776f9a3c39d36aaab\n",
            "[INFO|configuration_utils.py:566] 2021-07-15 19:32:25,590 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"conversational\": {\n",
            "      \"max_length\": 1000\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:1161] 2021-07-15 19:32:25,938 >> loading weights file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/aeb12aa1fc2f135700fcf9f8f0eec86c0649dc5ce0df86677adf0388271f33f3.1010e0ba25016a38144b58e8852f1dcc18876341e3b5728a99b3ffa11cc733cd\n",
            "[INFO|modeling_utils.py:1345] 2021-07-15 19:32:32,305 >> All model checkpoint weights were used when initializing GPT2DoubleHeadsModel.\n",
            "\n",
            "[WARNING|modeling_utils.py:1348] 2021-07-15 19:32:32,305 >> Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at microsoft/DialoGPT-small and are newly initialized: ['multiple_choice_head.summary.bias', 'multiple_choice_head.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/3cf340c89a43b5e6f31c4cd609fc2fc92f3d7aafdf6c8987e2ea9e02cb78b4e2.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/4e3f74e7c741909c4d1b48a23febe75c1be66a20c2b98cf7db4b8b10f12dc10c.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/5f8cf488e0bdda2e393e798f478673a4d26c1386082a1a05e42269f3ecc89f50.67d01b18f2079bd75eac0b2f2e7235768c7f26bd728e7a855a1c5acae01a91a8\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-15 19:32:34,402 >> loading file https://huggingface.co/microsoft/DialoGPT-small/resolve/main/tokenizer.json from cache at None\n",
            "07/15/2021 19:32:36 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-5d63192829ee3150.arrow\n",
            "07/15/2021 19:32:38 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-db7fb0dc7299ea14.arrow\n",
            "07/15/2021 19:32:39 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-7f1ea734015d543a.arrow\n",
            "07/15/2021 19:32:40 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-ec6932e5728037ea.arrow\n",
            "07/15/2021 19:32:42 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-f0949262d2915fc6.arrow\n",
            "07/15/2021 19:32:44 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-76567f244b20aa95.arrow\n",
            "07/15/2021 19:32:45 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-23db8b3b91274167.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-6029bb33ad67170c.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-707513ca871cc1bb.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-ca712ec39fe6eaa4.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-87b4f2e5ca7e08d2.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-0529b7ce2cd3e07a.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-92120f06a3fd9522.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-6b324cf97bcc04d6.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-442d13038a847e39.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-43e9e8e52cac0e3f.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-2cc34501fc868167.arrow\n",
            "07/15/2021 19:32:46 - WARNING - datasets.arrow_dataset -   Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/json/default-1f1027bf6e73f1ee/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02/cache-cdb246ef9a2c1db7.arrow\n",
            "[INFO|trainer.py:521] 2021-07-15 19:32:51,171 >> The following columns in the training set  don't have a corresponding argument in `GPT2DoubleHeadsModel.forward` and have been ignored: overflow.\n",
            "[INFO|trainer.py:282] 2021-07-15 19:32:51,187 >> ***** Running training *****\n",
            "[INFO|trainer.py:283] 2021-07-15 19:32:51,188 >>   Num examples = 25217\n",
            "[INFO|trainer.py:284] 2021-07-15 19:32:51,188 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:286] 2021-07-15 19:32:51,188 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:289] 2021-07-15 19:32:51,188 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:292] 2021-07-15 19:32:51,188 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:294] 2021-07-15 19:32:51,188 >>   Total optimization steps = 75651\n",
            "[INFO|integrations.py:409] 2021-07-15 19:32:51,189 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33magiantwhale\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep\n",
            "2021-07-15 19:32:52.736586: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.10.33\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/tmp/hp_search\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/agiantwhale/jaegpt-jaegpt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🧹 View sweep at \u001b[34m\u001b[4mhttps://wandb.ai/agiantwhale/jaegpt-jaegpt/sweeps/mwynodig\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/agiantwhale/jaegpt-jaegpt/runs/s5d8hrdv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20210715_193251-s5d8hrdv\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'output_dir' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'overwrite_output_dir' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_train' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'do_eval' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'evaluation_strategy' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_train_batch_size' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'per_device_eval_batch_size' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'gradient_accumulation_steps' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'logging_strategy' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'save_strategy' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'seed' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'dataloader_num_workers' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'report_to' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'mc_task_weight' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'lm_task_weight' was locked by 'sweep' (ignored update).\n",
            " 33% 25217/75651 [29:23<56:37, 14.84it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            " 67% 50433/75651 [58:53<29:12, 14.39it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            " 79% 59389/75651 [1:09:16<18:21, 14.76it/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSRSeEJ6lb1v"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}