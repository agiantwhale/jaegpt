project: dialo-fbgpt
program: run_clm.py
method: bayes
metric:
  goal: minimize
  name: eval/lm_loss
parameters:
  run_name:
    value: 'sweep-run'
  train_file:
    value: ./fb_data/sanitized/fb_train.json
  validation_file:
    value: ./fb_data/sanitized/fb_test.json
  output_dir:
    value: ./models/hp_search
  overwrite_output_dir:
    value: true
  per_device_eval_batch_size:
    value: 4
  preprocessing_num_workers:
    value: 4
  dataloader_num_workers:
    value: 4
  block_size:
    value: 128
  model_name_or_path:
    value: microsoft/DialoGPT-small
  report_to:
    value: 'wandb'
  do_train:
    value: true
  do_eval:
    value: true
  seed:
    value: 1337
  lm_task_weight:
    value: 1.0
  logging_strategy:
    value: 'no'
  evaluation_strategy:
    value: 'no'
  save_strategy:
    value: 'no'
  num_train_epochs:
    max: 4
    min: 1
    distribution: int_uniform
  gradient_accumulation_steps:
    values: 
    - 1
    - 2
    - 4
    - 8
    - 16
  per_device_train_batch_size:
    values: 
    - 1
    - 2
    - 4
    - 8
  mc_task_weight:
    max: 5.0
    min: -10.0
    distribution: log_uniform
  learning_rate:
    mu: -10
    sigma: 3.33
    distribution: log_normal