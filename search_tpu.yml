project: dialo-fbgpt
program: run_clm_tpu.py
method: bayes
metric:
  goal: minimize
  name: eval/lm_loss
parameters:
  run_name:
    value: 'sweep-run'
  train_file:
    value: /content/drive/MyDrive/ColabData/input_data/fb_train.json
  validation_file:
    value: /content/drive/MyDrive/ColabData/input_data/fb_test.json
  output_dir:
    value: /tmp/hp_search
  overwrite_output_dir:
    value: true
  gradient_accumulation_steps:
    value: 1
  per_device_eval_batch_size:
    value: 16
  preprocessing_num_workers:
    value: 4
  dataloader_num_workers:
    value: 4
  block_size:
    value: 128
  model_name_or_path:
    value: microsoft/DialoGPT-small
  report_to:
    value: 'wandb'
  do_train:
    value: true
  do_eval:
    value: true
  seed:
    value: 1337
  lm_task_weight:
    value: 1.0
  logging_strategy:
    value: 'no'
  evaluation_strategy:
    value: 'no'
  save_strategy:
    value: 'no'
  num_train_epochs:
    max: 4
    min: 1
    distribution: int_uniform
  per_device_train_batch_size:
    values: 
    - 1
    - 4
    - 8
    - 16
  mc_task_weight:
    max: 7.0
    min: -7.0
    distribution: log_uniform
  learning_rate:
    max: 0
    min: -25
    distribution: log_uniform
command:
  - ${env}
  - python
  - ${program}
  - "--num_cores"
  - 8
  - ${args}